#CSR = #sparse_tensor.encoding<{ map = (d0, d1) -> (d0: dense, d1: compressed) }>

module {

func.func @spmv(%vec: tensor<{{ cols }}xf64>,
                %res: tensor<{{ rows }}xf64>,
                %mat: tensor<{{ rows }}x{{ cols }}xf64, #CSR>) -> tensor<{{ rows }}xf64> {

    %num_of_rows = arith.constant {{ rows }} : index
                {% for index in indices %}
    %c{{ index }} = arith.constant {{ index }} : index
                {% endfor %}

    %pos = sparse_tensor.positions %mat {level = 1 : index} : tensor<{{ rows }}x{{ cols }}xf64, #CSR> to memref<?xindex>
    %crd = sparse_tensor.coordinates %mat {level = 1 : index} : tensor<{{ rows }}x{{ cols }}xf64, #CSR> to memref<?xindex>
    %mat_vals = sparse_tensor.values %mat : tensor<{{ rows }}x{{ cols }}xf64, #CSR> to memref<?xf64>

    %vec_vals = bufferization.to_memref %vec : memref<{{ cols }}xf64>
    %res_buff = bufferization.to_memref %res : memref<{{ rows }}xf64>

    // micro params
    %cl_size_in_indices = arith.constant {{ cl_size_in_indices }} : index
    %l1d_mshrs = arith.constant {{ l1d_mshrs }} : index
    %l2_mshrs = arith.constant {{ l2_mshrs }} : index

    scf.for %i = %c0 to %num_of_rows step %c1 {
        %init_res_i = memref.load %res_buff[%i] : memref<{{ rows }}xf64>

        // Load j_start
        %j_start = memref.load %pos[%i] : memref<?xindex>

        // Load j_end
        %i_plus_one = arith.addi %i, %c1 : index
        %j_end = memref.load %pos[%i_plus_one] : memref<?xindex>

        // Need to split loop [j_start, j_end) to 2 parts:
        // 1. [j_start, j_max_imul_of_cl_size) that has a step of cl_size_in_indices
        // 2. [j_mul_of_cl_size, j_end) with step 1
        %num_of_nnz = arith.subi %j_end, %j_start : index
        %num_of_nnz_DIV_cl_size_in_indices = arith.divui %num_of_nnz, %cl_size_in_indices : index
        %greatest_int_mult_of_cl_size_less_than_num_of_nnz = arith.muli %num_of_nnz_DIV_cl_size_in_indices, %cl_size_in_indices : index
        %j_end_unrolled = arith.addi %j_start, %greatest_int_mult_of_cl_size_less_than_num_of_nnz : index

        %partial_res_i = scf.for %j = %j_start to %j_end_unrolled step %cl_size_in_indices iter_args(%acc = %init_res_i) -> (f64) {

                {% for index in indices %}
            // Unrolled: iteration {{ index }}
            %j_plus_{{ index }} = arith.addi %j, %c{{ index }} : index
            %col_{{ index }} = memref.load %crd[%j_plus_{{ index }}] : memref<?xindex>
            %mat_{{ index }} = memref.load %mat_vals[%j_plus_{{ index }}] : memref<?xf64>
            %vec_val_{{ index }} = memref.load %vec_vals[%col_{{ index }}] : memref<{{ cols }}xf64>

            %mul_{{ index }} = arith.mulf %mat_{{ index }}, %vec_val_{{ index }} : f64
                    {% if index == 0 %}
            %acc_{{ index }} = arith.addf %acc, %mul_{{ index }} : f64
                    {% else %}
            %acc_{{ index }} = arith.addf %acc_{{ index - 1 }}, %mul_{{ index }} : f64
                    {% endif %}
                {% endfor %}

            scf.yield %acc_{{ cl_size_in_indices -  1 }} : f64
        }

        %res_i = scf.for %j = %j_end_unrolled to %j_end step %c1 iter_args(%acc = %partial_res_i) -> (f64) {

            // Load column indices and values for non-zero elements
            %col_idx = memref.load %crd[%j] : memref<?xindex>
            %mat_j = memref.load %mat_vals[%j] : memref<?xf64>
            %vec_val = memref.load %vec_vals[%col_idx] : memref<{{ cols }}xf64>

            // Compute
            %mul = arith.mulf %mat_j, %vec_val : f64
            %partial_res = arith.addf %acc, %mul : f64

            scf.yield %partial_res : f64
        }

        memref.store %res_i, %res_buff[%i] : memref<{{ rows }}xf64>
    }

    %5 = bufferization.to_tensor %res_buff restrict : memref<{{ rows }}xf64>
    return %5 : tensor<{{ rows }}xf64>
}

            {% include "main.mlir.jinja2" %}
}
