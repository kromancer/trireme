From c8b0ccd8e1daf58a155d45add432ce8df570b0f7 Mon Sep 17 00:00:00 2001
From: Konstantinos Sotiropoulos <konstantinos.sotiropoulos@chalmers.se>
Date: Fri, 3 Jan 2025 10:42:07 +0100
Subject: [PATCH 1/3] [mlir][sparse] Add prefetching for irregular accesses

---
 .../Dialect/SparseTensor/Pipelines/Passes.h   |  6 +-
 .../Dialect/SparseTensor/Transforms/Passes.h  | 10 ++--
 .../Dialect/SparseTensor/Transforms/Passes.td |  2 +
 .../Transforms/SparseTensorPasses.cpp         |  3 +-
 .../Transforms/Utils/CodegenEnv.cpp           |  4 +-
 .../Transforms/Utils/CodegenEnv.h             |  2 +-
 .../Transforms/Utils/LoopEmitter.cpp          | 59 ++++++++++++++++++-
 .../Transforms/Utils/LoopEmitter.h            |  7 ++-
 .../Transforms/Utils/SparseTensorIterator.cpp | 41 ++++++++++++-
 .../Transforms/Utils/SparseTensorIterator.h   |  6 ++
 10 files changed, 125 insertions(+), 15 deletions(-)

diff --git a/mlir/include/mlir/Dialect/SparseTensor/Pipelines/Passes.h b/mlir/include/mlir/Dialect/SparseTensor/Pipelines/Passes.h
index efbe5c56a219..14ab25c4ee9f 100644
--- a/mlir/include/mlir/Dialect/SparseTensor/Pipelines/Passes.h
+++ b/mlir/include/mlir/Dialect/SparseTensor/Pipelines/Passes.h
@@ -93,6 +93,10 @@ struct SparsifierOptions : public PassPipelineOptions<SparsifierOptions> {
       *this, "vl", desc("Set the vector length (0 disables vectorization)"),
       init(0)};
 
+  PassOptions::Option<int32_t> prefetchDistance{
+      *this, "pd", desc("Set the prefetch distance (0 disables prefetching)"),
+      init(0)};
+
   // These options must be kept in sync with the `ConvertVectorToLLVM`
   // (defined in include/mlir/Dialect/SparseTensor/Pipelines/Passes.h).
   PassOptions::Option<bool> reassociateFPReductions{
@@ -158,7 +162,7 @@ struct SparsifierOptions : public PassPipelineOptions<SparsifierOptions> {
   /// Projects out the options for `createSparsificationPass`.
   SparsificationOptions sparsificationOptions() const {
     return SparsificationOptions(parallelization, emitStrategy,
-                                 enableRuntimeLibrary);
+                                 enableRuntimeLibrary, prefetchDistance);
   }
 
   /// Projects out the options for `createConvertVectorToLLVMPass`.
diff --git a/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.h b/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.h
index 2e9c297f2018..0f21310ba369 100644
--- a/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.h
+++ b/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.h
@@ -92,20 +92,22 @@ std::unique_ptr<Pass> createPreSparsificationRewritePass();
 /// Options for the Sparsification pass.
 struct SparsificationOptions {
   SparsificationOptions(SparseParallelizationStrategy p, SparseEmitStrategy d,
-                        bool enableRT)
+                        bool enableRT, int32_t prefDist)
       : parallelizationStrategy(p), sparseEmitStrategy(d),
-        enableRuntimeLibrary(enableRT) {}
+        enableRuntimeLibrary(enableRT), prefetchDistance(prefDist){}
 
   SparsificationOptions(SparseParallelizationStrategy p, bool enableRT)
-      : SparsificationOptions(p, SparseEmitStrategy::kFunctional, enableRT) {}
+      : SparsificationOptions(p, SparseEmitStrategy::kFunctional,
+			      enableRT, 0) {}
 
   SparsificationOptions()
       : SparsificationOptions(SparseParallelizationStrategy::kNone,
-                              SparseEmitStrategy::kFunctional, true) {}
+                              SparseEmitStrategy::kFunctional, true, 0) {}
 
   SparseParallelizationStrategy parallelizationStrategy;
   SparseEmitStrategy sparseEmitStrategy;
   bool enableRuntimeLibrary;
+  int32_t prefetchDistance;
 };
 
 /// Sets up sparsification rewriting rules with the given options.
diff --git a/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.td b/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.td
index a534381bd5c2..9bf23ce02afe 100644
--- a/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.td
+++ b/mlir/include/mlir/Dialect/SparseTensor/Transforms/Passes.td
@@ -182,6 +182,8 @@ def SparsificationPass : Pass<"sparsification", "ModuleOp"> {
                         "Emit non-functional but easy-to-read interfaces to debug."))}]>,
     Option<"enableRuntimeLibrary", "enable-runtime-library", "bool",
            "true", "Enable runtime library for manipulating sparse tensors">,
+    Option<"prefetchDistance", "pd", "int32_t",
+           "0", "Emit (experimental) prefetches for indirect memory accesses (use 0 to disable)">,
   ];
 }
 
diff --git a/mlir/lib/Dialect/SparseTensor/Transforms/SparseTensorPasses.cpp b/mlir/lib/Dialect/SparseTensor/Transforms/SparseTensorPasses.cpp
index 153b9b170e5d..29544d5076eb 100644
--- a/mlir/lib/Dialect/SparseTensor/Transforms/SparseTensorPasses.cpp
+++ b/mlir/lib/Dialect/SparseTensor/Transforms/SparseTensorPasses.cpp
@@ -99,13 +99,14 @@ struct SparsificationPass
     parallelization = options.parallelizationStrategy;
     sparseEmitStrategy = options.sparseEmitStrategy;
     enableRuntimeLibrary = options.enableRuntimeLibrary;
+    prefetchDistance = options.prefetchDistance;
   }
 
   void runOnOperation() override {
     auto *ctx = &getContext();
     // Translate strategy flags to strategy options.
     SparsificationOptions options(parallelization, sparseEmitStrategy,
-                                  enableRuntimeLibrary);
+                                  enableRuntimeLibrary, prefetchDistance);
     // Apply sparsification and cleanup rewriting.
     RewritePatternSet patterns(ctx);
     populateSparsificationPatterns(patterns, options);
diff --git a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/CodegenEnv.cpp b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/CodegenEnv.cpp
index 86c13d03c7ec..0e37b4ca3c21 100644
--- a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/CodegenEnv.cpp
+++ b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/CodegenEnv.cpp
@@ -59,7 +59,7 @@ LogicalResult CodegenEnv::initTensorExp() {
   return success();
 }
 
-void CodegenEnv::startEmit(SparseEmitStrategy emitStrategy) {
+void CodegenEnv::startEmit(SparseEmitStrategy emitStrategy, int32_t prefetchDistance) {
   assert(insChain == nullptr && "must only start emitting once");
   if (sparseOut) {
     insChain = sparseOut->get();
@@ -97,7 +97,7 @@ void CodegenEnv::startEmit(SparseEmitStrategy emitStrategy) {
       [this](TensorId t, Level lvl) -> std::vector<LoopCoeffPair> {
         return merger().getDependentLoops(t, lvl);
       },
-      emitStrategy);
+      emitStrategy, prefetchDistance);
 }
 
 std::optional<Operation *> CodegenEnv::genLoopBoundary(
diff --git a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/CodegenEnv.h b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/CodegenEnv.h
index 34b793ee11e4..dc1d9e7d4fb4 100644
--- a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/CodegenEnv.h
+++ b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/CodegenEnv.h
@@ -56,7 +56,7 @@ public:
   Merger &merger() { return latticeMerger; }
   LoopEmitter &emitter() { return loopEmitter; }
 
-  void startEmit(SparseEmitStrategy emitStrategy);
+  void startEmit(SparseEmitStrategy emitStrategy, int32_t prefetchDistance);
 
   /// Generates loop boundary statements (entering/exiting loops). The function
   /// passes and updates the passed-in parameters.
diff --git a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/LoopEmitter.cpp b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/LoopEmitter.cpp
index ea5533dfc6ba..212d5b7c465d 100644
--- a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/LoopEmitter.cpp
+++ b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/LoopEmitter.cpp
@@ -116,19 +116,22 @@ static Value tryFoldTensors(Value t) {
 LoopEmitter::LoopEmitter(ValueRange tensors, StringAttr loopTag, bool hasOutput,
                          bool isSparseOut, unsigned numLoops,
                          DependentLvlGetter dimGetter,
-                         SparseEmitStrategy emitStrategy) {
+                         SparseEmitStrategy emitStrategy,
+                         int32_t prefetchDistance) {
   initialize(tensors, loopTag, hasOutput, isSparseOut, numLoops, dimGetter);
 }
 
 void LoopEmitter::initialize(ValueRange ts, StringAttr loopTag, bool hasOutput,
                              bool isSparseOut, unsigned numLoops,
                              DependentLvlGetter dimGetter,
-                             SparseEmitStrategy emitStrategy) {
+                             SparseEmitStrategy emitStrategy,
+                             int32_t prefetchDistance) {
   // First initialize the top-level type of the fields.
   this->loopTag = loopTag;
   this->hasOutput = hasOutput;
   this->isSparseOut = isSparseOut;
   this->emitStrategy = emitStrategy;
+  this->prefetchDistance = prefetchDistance;
 
   const unsigned numManifestTensors = ts.size();
   const unsigned synTensorId = numManifestTensors;
@@ -683,9 +686,59 @@ Operation *LoopEmitter::enterCoIterationOverTensorsAtLvls(
   }
 
   // Enter dense tensor levels.
-  for (SparseIterator *it : raIters)
+  for (SparseIterator *it : raIters) {
     it->locate(builder, loc, iv);
 
+    if (prefetchDistance == 0)
+      continue;
+
+    if (auto loadOp = dyn_cast_or_null<memref::LoadOp>(iv.getDefiningOp())) {
+      const auto indices = loadOp.getIndices();
+      assert(indices.size() == 1 && "Expected crd buffers to have one index");
+
+      assert(spIters.size() == 1 && "Expected only one sparse iterator");
+      Value upperB = spIters.front()->getMaxPos();
+
+      const auto crdBuf = loadOp.getMemRef();
+
+      // Find out if the random access iterator corresponds to the last level
+      // of the tensor
+      auto stt = getSparseTensorType(tensors[it->tid]);
+      const Level lvlRank = stt.getLvlRank();
+      const bool isLastLvl = it->lvl >= lvlRank - 1;
+
+      // Find out if the current loop is the innermost loop
+      // If not, use a prefetch distance of 1.
+      // The numOfLoops of the linalg op is implicit, via the capacity of the
+      // loopStack vector
+      auto prefDist =
+	C_IDX(getCurrentDepth() >= loopStack.capacity() - 1 ? prefetchDistance : 1);
+
+      // load A[min(i + distance, upperBound)]
+      auto plusDist = ADDI(indices[0], prefDist);
+      auto cmp = CMPI(ult, plusDist, upperB);
+      Value loadIdx = SELECT(cmp, plusDist, upperB).getResult();
+      auto prefIdx = sparse_tensor::genIndexLoad(builder, loc, crdBuf, loadIdx);
+
+      // Prefetch B[A[i]]
+      SmallVector<Value> prefIndices;
+      bool isWrite = false;
+      Value posOrValbuf;
+      if (isLastLvl) {
+        posOrValbuf = getValBuffer()[it->tid];
+        if (isOutputTensor(it->tid))
+          isWrite = true;
+        SmallVector<Value> valPosits = getValPosits(it->tid);
+        prefIndices.append(valPosits.begin(), valPosits.begin() + it->lvl);
+      } else {
+        const auto &stl = *lvls[it->tid][it->lvl + 1];
+        posOrValbuf = stl.getLvlBuffers()[0]; // Pos buffer
+      }
+      prefIndices.push_back(prefIdx);
+      builder.create<memref::PrefetchOp>(loc, posOrValbuf, prefIndices, isWrite, 2, true);
+    }
+  }
+
   // NOTE: we can also prepare for next dim here in advance
   // Pushes the loop into stack.
   loopStack.emplace_back(tls, l, builder.getInsertionBlock(), iv, loopTag);
diff --git a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/LoopEmitter.h b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/LoopEmitter.h
index 3e61b5f27fcc..dfde00fbc90b 100644
--- a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/LoopEmitter.h
+++ b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/LoopEmitter.h
@@ -89,13 +89,15 @@ public:
   initialize(ValueRange tensors, StringAttr loopTag = nullptr,
              bool hasOutput = false, bool isSparseOut = false,
              unsigned numLoops = 0, DependentLvlGetter getter = nullptr,
-             SparseEmitStrategy emitStrategy = SparseEmitStrategy::kFunctional);
+             SparseEmitStrategy emitStrategy = SparseEmitStrategy::kFunctional,
+             int32_t prefetchDistance = 0);
 
   explicit LoopEmitter(
       ValueRange tensors, StringAttr loopTag = nullptr, bool hasOutput = false,
       bool isSparseOut = false, unsigned numLoops = 0,
       DependentLvlGetter getter = nullptr,
-      SparseEmitStrategy emitStrategy = SparseEmitStrategy::kFunctional);
+      SparseEmitStrategy emitStrategy = SparseEmitStrategy::kFunctional,
+      int32_t prefetchDistance = 0);
 
   /// Starts a loop emitting session by generating all the buffers needed
   /// for iterating over the tensors.
@@ -392,6 +394,7 @@ private:
   bool hasOutput;
   bool isSparseOut;
   SparseEmitStrategy emitStrategy;
+  int32_t prefetchDistance;
 
   //
   // Fields which have `numTensor` many entries.
diff --git a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/SparseTensorIterator.cpp b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/SparseTensorIterator.cpp
index ef95fcc84bd9..70729575015b 100644
--- a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/SparseTensorIterator.cpp
+++ b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/SparseTensorIterator.cpp
@@ -102,6 +102,11 @@ public:
     Value posLo = MULI(p, lvlSize);
     return {posLo, lvlSize};
   }
+
+  Value peekMaxPos(OpBuilder &b, Location l,
+                   const SparseIterator *parent) const override {
+    return lvlSize;
+  }
 };
 
 class BatchLevel : public SparseTensorLevel {
@@ -122,6 +127,11 @@ public:
     // No need to linearize the position for non-annotated tensors.
     return {C_IDX(0), lvlSize};
   }
+
+  Value peekMaxPos(OpBuilder &b, Location l,
+                   const SparseIterator *parent) const override {
+    return lvlSize;
+  }
 };
 
 class CompressedLevel : public SparseLevel</*hasPosBuf=*/true> {
@@ -168,6 +178,18 @@ public:
     ValueRange posRange = posRangeIf.getResults();
     return {posRange.front(), posRange.back()};
   }
+
+  Value peekMaxPos(OpBuilder &b, Location l,
+                   const SparseIterator *parent) const override {
+    if (parent == nullptr) {
+      return genIndexLoad(b, l, getPosBuf(), C_IDX(1));
+    }
+
+    SmallVector<Value> memCrd(parent->getBatchCrds());
+    Value posHi = parent->upperBound(b, l);
+    memCrd.push_back(posHi);
+    return genIndexLoad(b, l, getPosBuf(), memCrd);
+  }
 }; // namespace
 
 class LooseCompressedLevel : public SparseLevel</*hasPosBuf=*/true> {
@@ -190,6 +212,11 @@ public:
     Value pHi = genIndexLoad(b, l, getPosBuf(), memCrd);
     return {pLo, pHi};
   }
+
+  Value peekMaxPos(OpBuilder &b, Location l,
+                   const SparseIterator *parent) const override {
+    assert(false && "Not implemented");
+  }
 }; // namespace
 
 class SingletonLevel : public SparseLevel</*hasPosBuf=*/false> {
@@ -211,6 +238,12 @@ public:
     return {p, segHi};
   }
 
+  Value peekMaxPos(OpBuilder &b, Location l,
+                   const SparseIterator *parent) const override {
+    assert(parent && "A singleton level must have a parent");
+    return parent->getMaxPos();
+  }
+
   ValuePair
   collapseRangeBetween(OpBuilder &b, Location l, ValueRange batchPrefix,
                        std::pair<Value, Value> parentRange) const override {
@@ -235,6 +268,11 @@ public:
     Value posLo = MULI(parentPos.front(), C_IDX(n));
     return {posLo, ADDI(posLo, C_IDX(n))};
   }
+
+  Value peekMaxPos(OpBuilder &b, Location l,
+                   const SparseIterator *parent) const override {
+    assert(false && "Not implemented");
+  }
 };
 
 } // namespace
@@ -460,7 +498,7 @@ public:
     Value posLo;
     ValueRange batchPrefix = parent ? parent->getBatchCrds() : ValueRange{};
     std::tie(posLo, posHi) = stl.peekRangeAt(b, l, batchPrefix, pPos);
-
+    maxPos = stl.peekMaxPos(b, l, parent);
     seek({posLo, genSegmentHigh(b, l, posLo)});
   }
 
@@ -1314,6 +1352,7 @@ void TrivialIterator::genInitImpl(OpBuilder &b, Location l,
 
   ValueRange batchPrefix = parent ? parent->getBatchCrds() : ValueRange{};
   std::tie(posLo, posHi) = stl.peekRangeAt(b, l, batchPrefix, pPos, inPadZone);
+  maxPos = stl.peekMaxPos(b, l, parent);
   // Seek to the lowest position.
   seek(posLo);
 }
diff --git a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/SparseTensorIterator.h b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/SparseTensorIterator.h
index 642cb1afa156..40a9ce485673 100644
--- a/mlir/lib/Dialect/SparseTensor/Transforms/Utils/SparseTensorIterator.h
+++ b/mlir/lib/Dialect/SparseTensor/Transforms/Utils/SparseTensorIterator.h
@@ -53,6 +53,9 @@ public:
   peekRangeAt(OpBuilder &b, Location l, ValueRange batchPrefix,
               ValueRange parentPos, Value inPadZone = nullptr) const = 0;
 
+  virtual Value peekMaxPos(OpBuilder &b, Location l,
+                           const SparseIterator *parent) const = 0;
+
   virtual std::pair<Value, Value>
   collapseRangeBetween(OpBuilder &b, Location l, ValueRange batchPrefix,
                        std::pair<Value, Value> parentRange) const {
@@ -261,6 +264,8 @@ public:
   // it != it.end()).
   Value deref(OpBuilder &b, Location l);
 
+  Value getMaxPos() const { return maxPos; }
+
   // Actual Implementation provided by derived class.
   virtual void genInitImpl(OpBuilder &, Location, const SparseIterator *) = 0;
   virtual ValueRange forwardImpl(OpBuilder &b, Location l) = 0;
@@ -335,6 +340,7 @@ protected:
 public:
   const IterKind kind;     // For LLVM-style RTTI.
   const unsigned tid, lvl; // tensor level identifier.
+  Value maxPos;
 
 private:
   Value crd; // The sparse coordinate used to coiterate;
-- 
2.47.0

